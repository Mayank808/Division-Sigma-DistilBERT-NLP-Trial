{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OurEffortsforSigmaDivision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdRniQ_1M5o4",
        "colab_type": "text"
      },
      "source": [
        "# **NLP - Sentiment Analysis Division Sigma Competition (Our Efforts)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njw_Ji6iM-Ir",
        "colab_type": "text"
      },
      "source": [
        "## *1.0 Importing necessary libraries*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7vwsRQkQTJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f219d1f0-5993-469f-885d-cac8f1fe28ab"
      },
      "source": [
        "# Standard imports - sklearn and transformer imports will be handled later\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from google.colab import drive\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m2GYUEPNEIR",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Import and save Training Data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yilYb4BsRmNB",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c16ec27a-4a4a-4495-b9d7-c549bedae49e"
      },
      "source": [
        "from google.colab import files\n",
        "data_to_load = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e520d054-011f-41e3-92a6-8f3e754b961b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e520d054-011f-41e3-92a6-8f3e754b961b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving training_data.csv to training_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HexD9NW2TROi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "df = pd.read_csv(io.StringIO(data_to_load[\"training_data.csv\"].decode(\"utf-8\")))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkOazeYmTelc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We pickle the dataframe to make it easier to load for late uses.\n",
        "df.to_pickle('train-df.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwTsqfN2TaV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_pickle('train-df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y1D_whGNgnW",
        "colab_type": "text"
      },
      "source": [
        "We define and use a quick lambda function for visualizations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYbthsE0NsGl",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Setting Up Sentiments Label and Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9P_durFTg2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Uses the sentiment reading in order to add a sentiment label to the data frame\n",
        "def encoded_to_label(sentiment_encode):\n",
        "  if sentiment_encode == 1:\n",
        "    return \"Positive\"\n",
        "  else:\n",
        "    return \"Negative\"\n",
        "\n",
        "df[\"Sentiment Label\"] = df[\"Sentiment\"].apply(lambda x: encoded_to_label(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMk73FlN_v6",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Displaying Data before Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZxPovoiNpsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "5bd9cd99-8a0d-48d4-dbe9-4a6b48390bc0"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>User</th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Sentiment Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>864192</td>\n",
              "      <td>Carly_FTS</td>\n",
              "      <td>I *heart* filling up @dennisschaub desk   1 it...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>523691</td>\n",
              "      <td>Open_Sourcing</td>\n",
              "      <td>#SocioMat - people create prettier, younger an...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>584154</td>\n",
              "      <td>xxcharlx</td>\n",
              "      <td>no way i dont want the tour to end</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1527961</td>\n",
              "      <td>andreapuddu</td>\n",
              "      <td>@HemalRadia Hi Amazing Brother! Sending Limitl...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>28609</td>\n",
              "      <td>umbec</td>\n",
              "      <td>@flockmaster they are chocolate</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999995</th>\n",
              "      <td>1366175</td>\n",
              "      <td>b13thy</td>\n",
              "      <td>@midderhonz i'm good.. off to buy an electric ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999996</th>\n",
              "      <td>681828</td>\n",
              "      <td>HeyyitsALison</td>\n",
              "      <td>@StaceyPaha i know..for youngerr boys..what am...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999997</th>\n",
              "      <td>488988</td>\n",
              "      <td>sleepycove</td>\n",
              "      <td>I can't belive it I just got asked for an auto...</td>\n",
              "      <td>1</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999998</th>\n",
              "      <td>985613</td>\n",
              "      <td>AmyyXD</td>\n",
              "      <td>i am putting my bb in the fridge so it cant di...</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999999</th>\n",
              "      <td>1410994</td>\n",
              "      <td>esmeretta</td>\n",
              "      <td>lexo wont come visit me</td>\n",
              "      <td>0</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             ID           User  ... Sentiment  Sentiment Label\n",
              "0        864192      Carly_FTS  ...         1         Positive\n",
              "1        523691  Open_Sourcing  ...         1         Positive\n",
              "2        584154       xxcharlx  ...         0         Negative\n",
              "3       1527961    andreapuddu  ...         1         Positive\n",
              "4         28609          umbec  ...         1         Positive\n",
              "...         ...            ...  ...       ...              ...\n",
              "999995  1366175         b13thy  ...         1         Positive\n",
              "999996   681828  HeyyitsALison  ...         1         Positive\n",
              "999997   488988     sleepycove  ...         1         Positive\n",
              "999998   985613         AmyyXD  ...         0         Negative\n",
              "999999  1410994      esmeretta  ...         0         Negative\n",
              "\n",
              "[1000000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYyNpCi4Ti8S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2bcc3231-84ea-4153-9296-015a67b0a550"
      },
      "source": [
        "# Checking for empty values\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 5 columns):\n",
            " #   Column           Non-Null Count    Dtype \n",
            "---  ------           --------------    ----- \n",
            " 0   ID               1000000 non-null  int64 \n",
            " 1   User             1000000 non-null  object\n",
            " 2   Text             1000000 non-null  object\n",
            " 3   Sentiment        1000000 non-null  int64 \n",
            " 4   Sentiment Label  1000000 non-null  object\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 38.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-Z0YvOKOHHJ",
        "colab_type": "text"
      },
      "source": [
        "## *2.0 Preprocessing Data*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWs9HbzYOlD4",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Formating The Text for the Bert or DistilBert Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oujpHecFTkkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful libraries\n",
        "import re \n",
        "import unicodedata \n",
        "import string\n",
        "\n",
        "# Removing punctuation with regular expressions\n",
        "def clean_sentence(text):\n",
        "    text = unicodedata.normalize('NFKD', text)\n",
        "    text = text.strip(', , ,')\n",
        "    text = text.strip('\\t\\n') \n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKhB5zBGTna-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the data cleaning to the pandas dataframe\n",
        "df_revised =  pd.DataFrame(df['Text'].apply(lambda x: clean_sentence(x)))\n",
        "df_revised['Sentiment'] = df['Sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcxQ_hyBTqZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before we begin encoding the text, we need to split it up into a training and validation set\n",
        "# We use scikit-learn's train_test_split for this\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, valid_df = train_test_split(df_revised, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOkeRCMpTw7e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "1f0e3494-a0d8-491e-ecbd-b34d8b4156d8"
      },
      "source": [
        "# Necessary pip command to use HuggingFace's transformer library and toolkit\n",
        "!pip install transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 4.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 23.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.18.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=361ebd0cd58b051e429c233fbbfd6ab0f6448afbfe35aff9aa636368736331c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2uGpnFOc23",
        "colab_type": "text"
      },
      "source": [
        "Before we feed the data to the BERT or DistilBert model, we must preprocess it. First, we must tokenize the data, add start-of-sentence and end-of-sentence tokens, pad the data, and mask it. This will be done with the transformer tokenizer's batch_encode_plus(method), which enables us to do all of this at once.\n",
        "Then, we must"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtjs2z7HOjh5",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Tokenizing, Padding, and Masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G-l5QSST1hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Necessary imports\n",
        "import transformers \n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        " \n",
        "test = 160\n",
        "\n",
        " # Using the names of the available models from the pretrained_models documentation\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "# To silence Colab warnings\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "# Encoding the data\n",
        "def encode_data(text, tokenizer, testlen=160): \n",
        "  encoded =  tokenizer.batch_encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      return_attention_masks=True, \n",
        "      return_token_type_ids=False,   \n",
        "      pad_to_max_length=True,\n",
        "      max_length=test\n",
        "  ) \n",
        "  return np.array(encoded['input_ids'])\n",
        "\n",
        "# Applying the encoding to the training and validation data\n",
        "train_encoded_data = encode_data(train_df.Text.values, tokenizer, testlen = 160)\n",
        "valid_encoded_data = encode_data(valid_df.Text.values, tokenizer, testlen = 160)  "
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf5zpbIPPL6u",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Converting to Tensorflow Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAoFY8XuPKxu",
        "colab_type": "text"
      },
      "source": [
        "We convert the pandas series that were output previously and convert it to TensorFlow datasets with the from_tensor_slices(method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUhw0SYVUdOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = train_df.Sentiment.values\n",
        "y_valid = valid_df.Sentiment.values\n",
        " \n",
        "BATCH_SIZE = 32\n",
        "\n",
        "ds_train = (tf.data.Dataset \n",
        ".from_tensor_slices((train_encoded_data, y_train)) \n",
        ".repeat()\n",
        ".shuffle(512)\n",
        ".batch(BATCH_SIZE)\n",
        ".prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "\n",
        "ds_valid = (tf.data.Dataset \n",
        ".from_tensor_slices((valid_encoded_data, y_valid)) \n",
        ".repeat()\n",
        ".shuffle(512)\n",
        ".batch(BATCH_SIZE)\n",
        ".prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pumWW7MPbH2",
        "colab_type": "text"
      },
      "source": [
        "## *3.0 Training the Model*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVzK0-G6PgWw",
        "colab_type": "text"
      },
      "source": [
        "The Bert model training came out to about 300+ hours for us for some reason and we tried to spend a very long time to fix it but we werent able to. Alot of PAIN :( We decided to change the model to distilbert however the training was still way to long at about 4+hours per epoch. This is weird for even the distilbert model as it should be able to be trained fully in under an hour for this training data set. We tried our best to cut down the training time but kept getting training errors at the end of when an epoch was about to finish. Trying to avoid the training errors we also added checkpoints but those didn't end up working lol!!!! In the end we tried our best and even though we werent able to succeed we managed to learn alot about NLP models and their implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joexFOxTVQOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #setting up checkpoint call backs for the modes training \n",
        " checkpoint_cb = keras.callbacks.ModelCheckpoint('sigma_bert_model.h5', save_best_only=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJcyl8ewRbUv",
        "colab_type": "text"
      },
      "source": [
        "Implementing the distilbert model after the bert model didnt work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw3LgFT4VTAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "model=TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased') \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FodM9wsIRiDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Distilbert model summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGgvBYkeV442",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "38672e8c-66e7-4dd6-de5f-36dc97720b8d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_distil_bert_for_sequence_classification_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "distilbert (TFDistilBertMain multiple                  65190912  \n",
            "_________________________________________________________________\n",
            "pre_classifier (Dense)       multiple                  590592    \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "_________________________________________________________________\n",
            "dropout_139 (Dropout)        multiple                  0         \n",
            "=================================================================\n",
            "Total params: 65,783,042\n",
            "Trainable params: 65,783,042\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fcv5r-pVW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment the next line to restore the checkpoint\n",
        "#This sadly didnt end up working :(\n",
        "# model = keras.models.load_model('sigma_bert_model.ht')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9BgaaOYRtc6",
        "colab_type": "text"
      },
      "source": [
        "Tried to train the data below, but due to the long training times either human errors would happen or the training would just not work after one epoch. Spent a long time tring to change the hyperparamters but only the base model was properly training for atleast one epoch before breaking. PAIN :("
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UEaqm1VnaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_steps = 800000/BATCH_SIZE\n",
        "history = model.fit(ds_train, steps_per_epoch = number_steps, validation_data = ds_valid, epochs = 3, callbacks = [checkpoint_cb]) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}